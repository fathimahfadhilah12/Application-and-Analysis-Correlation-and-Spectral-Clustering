# -*- coding: utf-8 -*-
"""Analysis Correlation and Cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hdybCxJtEKVNkRBUh-g45-qR7fJqhnEe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import SpectralClustering
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import silhouette_score
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import rbf_kernel
from sklearn.manifold import SpectralEmbedding
from sklearn.metrics.pairwise import rbf_kernel

# Path ke file Excel
file_path = "Data Pengangguran_Prakerja.xlsx"

# Pastikan menggunakan 'openpyxl' sebagai engine
df = pd.read_excel(file_path, engine='openpyxl', dtype=str)
print(df.head())

##Missing Value Handling untuk mengecek data csv terdapat missing value atau tidak
# Cek jumlah NaN awal
print("Missing Value:")
print(df.isnull().sum())

# Ganti "-" dengan NaN (supaya tidak error saat transformasi)
df.replace("-", np.nan, inplace=True)

# Drop kolom yang lebih dari 90% datanya kosong (jika ada)
df.dropna(axis=1, thresh=0.1 * len(df), inplace=True)
df.columns = df.columns.str.strip()  # Menghapus spasi di awal/akhir nama kolom

# Imputasi untuk kolom numerik menggunakan rata-rata (mean)
imputer = SimpleImputer(strategy="mean")
df[['SD ke bawah', 'Diploma I/II/III/Akademi/Universitas']] = imputer.fit_transform(df[['SD ke bawah', 'Diploma I/II/III/Akademi/Universitas']])

# Cek missing values setelah imputasi
print("✅ Data setelah imputasi missing values:")
print(df.isnull().sum())

print(df.duplicated().sum())  # Melihat jumlah data duplikat

# Salin dataset
X = df.copy()

# Hapus spasi di awal/akhir nama kolom
X.columns = X.columns.str.strip()

# Label Encoding Wilayah
if "Wilayah" in X.columns:
    label_encoder = LabelEncoder()
    X["Wilayah_Encoded"] = label_encoder.fit_transform(X["Wilayah"])  # Simpan hasil encoding

# Konversi kolom numerik ke tipe float
numeric_columns = [
    "Jumlah angkatan Kerja", "Jumlah pengangguran terbuka", "TPT", "laki-laki", "perempuan",
    "SD ke bawah", "SMP", "SMA", "SMK", "Diploma I/II/III/Akademi/Universitas", "sk penetapan","Aktif"
]
# Fungsi membersihkan nilai numerik tanpa pembulatan
def clean_numeric(col):
    return (X[col]
            .astype(str)
            .str.replace(',', '', regex=True)  # Hapus koma pemisah ribuan
            .replace('-', np.nan)  # Ganti '-' dengan NaN jika ada
            .astype(float)  # Ubah ke float agar desimal tetap terbaca
           )

# Menerapkan pembersihan ke semua kolom numerik yang tersedia di dataset
for col in numeric_columns:
    if col in X.columns:
        try:
            X[col] = clean_numeric(col)
        except ValueError as e:
            print(f"Error pada kolom {col}: {e}")  # Deteksi error jika ada

# Konversi kolom 'Tahun', 'TPAK', dan 'TPT' menjadi numerik
X['Tahun'] = pd.to_numeric(X['Tahun'], errors='coerce')  # Ganti yang tidak bisa diubah jadi NaN
X['TPAK'] = pd.to_numeric(X['TPAK'], errors='coerce')  # Sama seperti di atas
X['TPT'] = pd.to_numeric(X['TPT'], errors='coerce')  # Sama seperti di atas

# Pastikan kolom Wilayah tetap ada
print(X.info())  # Cek tipe data setelah konversi

X_prakerja = df[[
    "TPT", "Jumlah pengangguran terbuka", "Aktif","TPAK", "Jumlah angkatan Kerja",
    "laki-laki", "perempuan",  "SD ke bawah", "SMP", "SMA", "SMK", "Diploma I/II/III/Akademi/Universitas"
]]

# Inisialisasi MinMaxScaler
scaler = MinMaxScaler()

# Lakukan normalisasi pada X_prakerja
X_prakerja_ready = scaler.fit_transform(X_prakerja)

# Mengonversi hasil normalisasi kembali ke DataFrame untuk kemudahan analisis
X_prakerja_ready_df = pd.DataFrame(X_prakerja_ready, columns=X_prakerja.columns)

# Menampilkan DataFrame hasil normalisasi
print(X_prakerja_ready_df.head())

## MEMBANGUN SIMILAR MATRIX
from sklearn.metrics.pairwise import pairwise_kernels

# Compute the affinity matrix using the Gaussian (RBF) kernel
gamma = 1.0  # The bandwidth parameter
affinity_matrix_gaussian = pairwise_kernels(X_prakerja_ready, metric='rbf', gamma=gamma)

# Display the Affinity Matrix
print("✅ Affinity Matrix (Gaussian Kernel):")
print(affinity_matrix_gaussian)

## MEMBANGUN Degree Matrix
# Degree Matrix (D) adalah jumlah kemiripan untuk setiap titik
degree_matrix = np.sum(affinity_matrix_gaussian, axis=1)

# Membuat matriks diagonal dari derajat
degree_matrix = np.diag(degree_matrix)

# Tampilkan Degree Matrix
print("✅ Degree Matrix (D):")
print(degree_matrix)

# Laplacian Matrix (L) = Degree Matrix (D) - Affinity Matrix (A)
laplacian_matrix = degree_matrix - affinity_matrix_gaussian

# Tampilkan Laplacian Matrix
print("✅ Laplacian Matrix (L):")
print(laplacian_matrix)

## EIGENVALUES DAN EIGENVECTOR
eigenvalues, eigenvectors = np.linalg.eig(laplacian_matrix)

from scipy.linalg import eigh

# 1. Hitung eigenvalues dan eigenvectors dari Laplacian
eigenvalues, eigenvectors = eigh(laplacian_matrix)

# 2. Ambil 2 eigenvector terkecil setelah nol (biasanya indeks 1 dan 2)
embedding_2d = eigenvectors[:, 1:3]

# 3. Cetak hasil
print("✅ 2D Spectral Embedding (dari Laplacian manual):")
print(np.round(embedding_2d, 4))

from scipy.linalg import eigh
from scipy.sparse import csgraph
from sklearn.metrics.pairwise import pairwise_kernels

# Compute the affinity matrix using the Gaussian (RBF) kernel
gamma = 1.0  # The bandwidth parameter
affinity_matrix_gaussian = pairwise_kernels(X_prakerja_ready, metric='rbf', gamma=gamma)

# 1. Hitung Laplacian: L = D - A
laplacian = csgraph.laplacian(affinity_matrix_gaussian, normed=True)

# 2. Hitung eigenvalues & eigenvectors
eigenvalues, eigenvectors = eigh(laplacian)

# 3. Ambil eigenvector ke-1 dan ke-2 (setelah 0)
embedding_2d = eigenvectors[:, 1:3]  # Skip yang index 0

# 4. Print dimensi dan hasilnya
print("Shape eigenvectors:", eigenvectors.shape)
print("Embedding 2D shape:", embedding_2d.shape)
print("Embedding 2D:")
print(embedding_2d)

# 5. Visualisasi
plt.figure(figsize=(6, 5))
plt.scatter(embedding_2d[:, 0], embedding_2d[:, 1], c='blue')
plt.title("Manual Spectral Embedding (2D via eigh)")
plt.xlabel("Eigenvector ke-2")
plt.ylabel("Eigenvector ke-3")
plt.grid(True)
plt.show()

# 1. Ambil k eigenvector teratas (misalnya 2 untuk dimensi 2D)
k = 2  # Pilih jumlah dimensi yang diinginkan untuk Spectral Clustering
eigenvector_subset = eigenvectors[:, :k]

# 2. Terapkan Spectral Clustering menggunakan eigenvectors yang telah dihitung
spectral_clustering = SpectralClustering( n_clusters=2, affinity='nearest_neighbors', random_state=42)

# 3. Melakukan klasterisasi
cluster_labels = spectral_clustering.fit_predict(eigenvector_subset)

# 4. Menambahkan hasil klasterisasi ke DataFrame
df['Cluster'] = cluster_labels

# 5. Menampilkan hasil klasterisasi beserta data lainnya
print("✅ Hasil Klasterisasi dengan Spectral Clustering:")
print(df[['Tahun', 'Wilayah', 'Cluster']])

df.to_csv('Analisis Pertahun.csv', index=False)

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.cluster import SpectralClustering

# Jalankan Spectral Clustering
spectral = SpectralClustering(
    n_clusters=2,
    affinity='nearest_neighbors',
    n_neighbors=10,
    random_state=42
)
labels = spectral.fit_predict(X_prakerja_ready)

# Reduksi dimensi ke 2D untuk visualisasi
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_prakerja_ready)

plt.figure(figsize=(8,6))

# Plot masing-masing cluster dengan warna & label
plt.scatter(
    X_pca[labels == 0, 0],
    X_pca[labels == 0, 1],
    color='gray',
    label='Cluster 0 (Abu-abu)',
    s=60,
    edgecolor='k'
)
plt.scatter(
    X_pca[labels == 1, 0],
    X_pca[labels == 1, 1],
    color='red',
    label='Cluster 1 (Merah)',
    s=60,
    edgecolor='k'
)

plt.title("Visualisasi Hasil Spectral Clustering (PCA 2D)")
plt.xlabel("PCA 1")
plt.ylabel("PCA 2")
plt.legend()
plt.grid(True)
plt.show()

# Menghitung Silhouette Score untuk mengevaluasi hasil klasterisasi
sil_score = silhouette_score(X_prakerja_ready, cluster_labels)

# Menampilkan Silhouette Score
print(f"✅ Silhouette Score: {sil_score:.4f}")

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import silhouette_score, silhouette_samples

# Misalnya: kamu sudah punya
# X_prakerja_ready = data hasil embedding / representasi 2D
# cluster_labels = hasil label klaster (misal dari KMeans atau Spectral Clustering)

# 1. Hitung Silhouette Score dan per sample
silhouette_avg = silhouette_score(X_prakerja_ready, cluster_labels)
sample_silhouette_values = silhouette_samples(X_prakerja_ready, cluster_labels)

# 2. Visualisasi Silhouette Plot
n_clusters = len(np.unique(cluster_labels))
fig, ax1 = plt.subplots(figsize=(8, 6))

y_lower = 10
for i in range(n_clusters):
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = plt.cm.nipy_spectral(float(i) / n_clusters)
    ax1.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    # Kode yang sudah diperbaiki
# Kode Perbaikan
    ax1.text(silhouette_avg + 0.02, y_lower + 0.5 * size_cluster_i, f"Cluster {i}")
    y_lower = y_upper + 10  # jarak antar cluster

# 3. Garis rata-rata Silhouette Score
ax1.axvline(x=silhouette_avg, color="red", linestyle="--", label=f"Avg Score = {silhouette_avg:.4f}")
ax1.set_title("Silhouette Plot untuk Evaluasi Klastering")
ax1.set_xlabel("Silhouette Coefficient")
ax1.set_ylabel("Cluster Label")
ax1.legend(loc='lower right')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import silhouette_score, silhouette_samples

# Misalnya: kamu sudah punya
# X_prakerja_ready = data hasil embedding / representasi 2D
# cluster_labels = hasil label klaster (misal dari KMeans atau Spectral Clustering)

# 1. Hitung Silhouette Score dan per sample
silhouette_avg = silhouette_score(X_prakerja_ready, cluster_labels)
sample_silhouette_values = silhouette_samples(X_prakerja_ready, cluster_labels)

# 2. Visualisasi Silhouette Plot
n_clusters = len(np.unique(cluster_labels))
fig, ax1 = plt.subplots(figsize=(8, 6))

y_lower = 10
for i in range(n_clusters):
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    color = plt.cm.nipy_spectral(float(i) / n_clusters)
    ax1.fill_betweenx(np.arange(y_lower, y_upper),
                      0, ith_cluster_silhouette_values,
                      facecolor=color, edgecolor=color, alpha=0.7)

    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, f"Cluster {i}")
    y_lower = y_upper + 10  # jarak antar cluster

# 3. Garis rata-rata Silhouette Score
ax1.axvline(x=silhouette_avg, color="red", linestyle="--", label=f"Avg Score = {silhouette_avg:.4f}")
ax1.set_title("Silhouette Plot untuk Evaluasi Klastering")
ax1.set_xlabel("Silhouette Coefficient")
ax1.set_ylabel("Cluster Label")
ax1.legend()
plt.tight_layout()
plt.show()

# Misalnya df sudah berisi kolom 'Cluster' yang sudah dihasilkan sebelumnya
# Kelompokkan data berdasarkan 'Wilayah' dan ambil modus untuk kolom 'Cluster'
df['Cluster'] = df.groupby('Wilayah')['Cluster'].transform(lambda x: x.mode()[0])

# Sekarang setiap wilayah hanya memiliki satu klaster berdasarkan modus
# Tampilkan hasilnya
print(df[['Wilayah', 'Cluster']].drop_duplicates())

# Ubah kolom numerik ke tipe float
kolom_numerik = [
    'TPT', 'Jumlah pengangguran terbuka', 'Aktif', 'TPAK', 'Jumlah angkatan Kerja',
    'laki-laki', 'perempuan', 'SD ke bawah', 'SMP', 'SMA', 'SMK',
    'Diploma I/II/III/Akademi/Universitas'
]

for kolom in kolom_numerik:
    df[kolom] = pd.to_numeric(df[kolom], errors='coerce')

# Menggabungkan data dan menghitung rata-rata semua kolom numerik
average_cluster_data = df.groupby(['Cluster', 'Wilayah', 'Tahun']).agg({
    'TPT': 'mean',
    'Jumlah pengangguran terbuka': 'mean',
    'Aktif': 'mean',
    'TPAK': 'mean',
    'Jumlah angkatan Kerja': 'mean',
    'laki-laki': 'mean',
    'perempuan': 'mean',
    'SD ke bawah': 'mean',
    'SMP': 'mean',
    'SMA': 'mean',
    'SMK': 'mean',
    'Diploma I/II/III/Akademi/Universitas': 'mean'
}).reset_index()

# Menampilkan hasil
print(average_cluster_data.head())

# Menyimpan ke CSV (rata-rata)
average_cluster_data.to_csv('Rata-rata prakerja & Pengangguran pertahun.csv', index=False)

# Ubah kolom numerik ke float
kolom_numerik = [
    'TPT', 'Jumlah pengangguran terbuka', 'Aktif', 'TPAK', 'Jumlah angkatan Kerja',
    'laki-laki', 'perempuan', 'SD ke bawah', 'SMP', 'SMA', 'SMK',
    'Diploma I/II/III/Akademi/Universitas'
]

for kolom in kolom_numerik:
    df[kolom] = pd.to_numeric(df[kolom], errors='coerce')

# Hitung rata-rata berdasarkan Cluster (0, 1, dst)
rata_rata_klaster = df.groupby('Cluster')[kolom_numerik].mean().reset_index()

# Tampilkan hasil
print(rata_rata_klaster)

# Membaca data
data_prakerja = pd.read_csv('Analisis Pertahun.csv')

# Menghitung korelasi antara 'Aktif' dan 'TPT'
kor_relasi = data_prakerja[['Aktif', 'TPT']].corr()

# Menampilkan hasil korelasi
print(kor_relasi)

# Visualisasi heatmap korelasi
plt.figure(figsize=(5, 4))
sns.heatmap(kor_relasi, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Korelasi Partisipasi Prakerja dan Tingkat Pengangguran (TPT)')
plt.tight_layout()
plt.show()

# Menghapus spasi ekstra di nama kolom
data_prakerja.columns = data_prakerja.columns.str.strip()

# Menghitung korelasi antara 'Aktif' dan 'TPT'
kor_relasi = data_prakerja[['Aktif', 'SD ke bawah', 'SMP', 'SMA', 'SMK', 'Diploma I/II/III/Akademi/Universitas']].corr()

# Menampilkan hasil korelasi
print(kor_relasi)

plt.figure(figsize=(8, 6))
sns.heatmap(kor_relasi, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Korelasi Partisipasi Prakerja dengan Jenjang Pendidikan')
plt.tight_layout()
plt.show()

# 1. Hitung rata-rata partisipasi aktif per wilayah
avg_aktif_per_wilayah = data_prakerja.groupby('Wilayah')['Aktif'].mean().reset_index()

# 2. Hitung median partisipasi
median_aktif = avg_aktif_per_wilayah['Aktif'].median()

# 3. Bagi wilayah berdasarkan partisipasi tinggi vs rendah
wilayah_tinggi = avg_aktif_per_wilayah[avg_aktif_per_wilayah['Aktif'] > median_aktif]['Wilayah']
wilayah_rendah = avg_aktif_per_wilayah[avg_aktif_per_wilayah['Aktif'] <= median_aktif]['Wilayah']

# 4. Filter data asli berdasarkan kelompok wilayah
data_tinggi = data_prakerja[data_prakerja['Wilayah'].isin(wilayah_tinggi)]
data_rendah = data_prakerja[data_prakerja['Wilayah'].isin(wilayah_rendah)]

# 5. Hitung rata-rata TPT tahun 2020 dan setelahnya
avg_tpt_tinggi_2020 = data_tinggi[data_tinggi['Tahun'] == 2020]['TPT'].mean()
avg_tpt_tinggi_after = data_tinggi[data_tinggi['Tahun'] >= 2021]['TPT'].mean()

avg_tpt_rendah_2020 = data_rendah[data_rendah['Tahun'] == 2020]['TPT'].mean()
avg_tpt_rendah_after = data_rendah[data_rendah['Tahun'] >= 2021]['TPT'].mean()

# 6. Cetak hasil
print(f"Rata-rata TPT partisipasi tinggi 2020: {avg_tpt_tinggi_2020:.2f}")
print(f"Rata-rata TPT partisipasi tinggi setelahnya: {avg_tpt_tinggi_after:.2f}\n")

print(f"Rata-rata TPT partisipasi rendah 2020: {avg_tpt_rendah_2020:.2f}")
print(f"Rata-rata TPT partisipasi rendah setelahnya: {avg_tpt_rendah_after:.2f}\n")

print("Wilayah dengan partisipasi tinggi:")
print(wilayah_tinggi.values)

print("\nWilayah dengan partisipasi rendah:")
print(wilayah_rendah.values)